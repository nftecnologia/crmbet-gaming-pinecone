"""
üìà Metrics Collector - INDUSTRIAL MONITORING
Advanced metrics collection with TB+/hour capability and real-time analytics

Author: Agente ETL Industrial - ULTRATHINK
Created: 2025-06-25
"""

import asyncio
import time
import psutil
import socket
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict
from enum import Enum
import structlog
import threading
import json

# Monitoring imports
from prometheus_client import (
    Counter, Histogram, Gauge, Summary, Info, Enum as PrometheusEnum,
    CollectorRegistry, generate_latest, start_http_server, CONTENT_TYPE_LATEST
)
import redis
import psutil
import os

# Network and HTTP imports
import aiohttp
import httpx

# Data processing for metrics
import pandas as pd
import numpy as np

logger = structlog.get_logger(__name__)

class MetricType(Enum):
    """Tipos de m√©tricas suportadas"""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"

class AlertLevel(Enum):
    """N√≠veis de alerta"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

@dataclass
class MetricDefinition:
    """Defini√ß√£o de uma m√©trica"""
    name: str
    description: str
    metric_type: MetricType
    labels: List[str] = field(default_factory=list)
    buckets: Optional[List[float]] = None  # Para histogramas
    objectives: Optional[Dict[float, float]] = None  # Para summaries

@dataclass
class MetricValue:
    """Valor de uma m√©trica com timestamp"""
    metric_name: str
    value: Union[int, float]
    labels: Dict[str, str] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class SystemMetrics:
    """M√©tricas do sistema"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used_gb: float
    memory_available_gb: float
    disk_usage_percent: float
    disk_free_gb: float
    network_bytes_sent: int
    network_bytes_recv: int
    load_average: List[float]
    
class MetricsCollector:
    """
    Coletor de m√©tricas industrial com capacidades avan√ßadas:
    - Coleta autom√°tica de m√©tricas de sistema
    - M√©tricas customizadas de aplica√ß√£o
    - Agrega√ß√£o em tempo real
    - Export para Prometheus, InfluxDB, etc.
    - Alertas baseados em thresholds
    - Performance tracking detalhado
    """
    
    def __init__(self, 
                 service_name: str = "crmbet-etl",
                 prometheus_port: int = 9090,
                 redis_client: Optional[redis.Redis] = None):
        
        self.service_name = service_name
        self.prometheus_port = prometheus_port
        self.logger = logger.bind(component="MetricsCollector", service=service_name)
        
        # Redis para m√©tricas distribu√≠das
        self.redis_client = redis_client or redis.Redis(host='localhost', port=6379, db=1)
        
        # Prometheus registry
        self.registry = CollectorRegistry()
        
        # M√©tricas internas
        self.custom_metrics: Dict[str, Any] = {}
        self.metric_definitions: Dict[str, MetricDefinition] = {}
        
        # Buffers para agrega√ß√£o
        self.metrics_buffer = deque(maxlen=10000)
        self.system_metrics_history = deque(maxlen=1000)
        
        # State tracking
        self.running = False
        self.collection_interval = 15  # segundos
        self.last_network_stats = None
        
        # Background tasks
        self.collection_task = None
        self.export_task = None
        self.aggregation_task = None
        
        # Initialize built-in metrics
        self._initialize_builtin_metrics()
        
        self.logger.info("MetricsCollector inicializado")
    
    def _initialize_builtin_metrics(self):
        """Inicializa m√©tricas built-in essenciais"""
        
        # ETL Pipeline metrics
        self.register_metric(MetricDefinition(
            name="etl_records_processed_total",
            description="Total records processed by ETL pipeline",
            metric_type=MetricType.COUNTER,
            labels=["pipeline", "stage", "status"]
        ))
        
        self.register_metric(MetricDefinition(
            name="etl_processing_duration_seconds",
            description="Time spent processing data",
            metric_type=MetricType.HISTOGRAM,
            labels=["pipeline", "stage"],
            buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 25.0, 50.0, 100.0]
        ))
        
        self.register_metric(MetricDefinition(
            name="etl_throughput_records_per_second",
            description="ETL throughput in records per second",
            metric_type=MetricType.GAUGE,
            labels=["pipeline"]
        ))
        
        self.register_metric(MetricDefinition(
            name="etl_memory_usage_bytes",
            description="Memory usage by ETL components",
            metric_type=MetricType.GAUGE,
            labels=["component"]
        ))
        
        self.register_metric(MetricDefinition(
            name="etl_error_rate",
            description="Error rate percentage",
            metric_type=MetricType.GAUGE,
            labels=["pipeline", "error_type"]
        ))
        
        # Data Quality metrics
        self.register_metric(MetricDefinition(
            name="data_quality_score",
            description="Data quality score (0-1)",
            metric_type=MetricType.GAUGE,
            labels=["dataset", "dimension"]
        ))
        
        self.register_metric(MetricDefinition(
            name="data_freshness_seconds",
            description="Data freshness in seconds",
            metric_type=MetricType.GAUGE,
            labels=["source"]
        ))
        
        # System metrics
        self.register_metric(MetricDefinition(
            name="system_cpu_percent",
            description="CPU usage percentage",
            metric_type=MetricType.GAUGE
        ))
        
        self.register_metric(MetricDefinition(
            name="system_memory_percent",
            description="Memory usage percentage",
            metric_type=MetricType.GAUGE
        ))
        
        self.register_metric(MetricDefinition(
            name="system_disk_usage_percent",
            description="Disk usage percentage",
            metric_type=MetricType.GAUGE,
            labels=["mount_point"]
        ))
        
        # Network metrics
        self.register_metric(MetricDefinition(
            name="network_bytes_total",
            description="Total network bytes",
            metric_type=MetricType.COUNTER,
            labels=["direction"]  # sent/received
        ))
        
        # Circuit breaker metrics
        self.register_metric(MetricDefinition(
            name="circuit_breaker_state",
            description="Circuit breaker state",
            metric_type=MetricType.GAUGE,
            labels=["service", "state"]
        ))
        
        # Kafka/Streaming metrics
        self.register_metric(MetricDefinition(
            name="kafka_messages_consumed_total",
            description="Total Kafka messages consumed",
            metric_type=MetricType.COUNTER,
            labels=["topic", "partition", "consumer_group"]
        ))
        
        self.register_metric(MetricDefinition(
            name="kafka_consumer_lag",
            description="Kafka consumer lag",
            metric_type=MetricType.GAUGE,
            labels=["topic", "partition", "consumer_group"]
        ))
        
        # Business metrics
        self.register_metric(MetricDefinition(
            name="business_users_processed_total",
            description="Total users processed",
            metric_type=MetricType.COUNTER,
            labels=["source"]
        ))
        
        self.register_metric(MetricDefinition(
            name="business_transactions_amount_total",
            description="Total transaction amounts processed",
            metric_type=MetricType.COUNTER,
            labels=["transaction_type", "currency"]
        ))
    
    def register_metric(self, definition: MetricDefinition):
        """Registra uma nova m√©trica"""
        
        self.metric_definitions[definition.name] = definition
        
        # Cria m√©trica Prometheus correspondente
        if definition.metric_type == MetricType.COUNTER:
            metric = Counter(
                definition.name,
                definition.description,
                definition.labels,
                registry=self.registry
            )
        elif definition.metric_type == MetricType.GAUGE:
            metric = Gauge(
                definition.name,
                definition.description,
                definition.labels,
                registry=self.registry
            )
        elif definition.metric_type == MetricType.HISTOGRAM:
            metric = Histogram(
                definition.name,
                definition.description,
                definition.labels,
                buckets=definition.buckets,
                registry=self.registry
            )
        elif definition.metric_type == MetricType.SUMMARY:
            metric = Summary(
                definition.name,
                definition.description,
                definition.labels,
                registry=self.registry
            )
        else:
            raise ValueError(f"Tipo de m√©trica n√£o suportado: {definition.metric_type}")
        
        self.custom_metrics[definition.name] = metric
        
        self.logger.debug("M√©trica registrada", name=definition.name, type=definition.metric_type.value)
    
    async def start_collection(self):
        """Inicia coleta autom√°tica de m√©tricas"""
        
        try:
            self.logger.info("Iniciando coleta de m√©tricas")
            self.running = True
            
            # Inicia servidor Prometheus
            start_http_server(self.prometheus_port, registry=self.registry)
            self.logger.info("Servidor Prometheus iniciado", port=self.prometheus_port)
            
            # Inicia tasks de background
            self.collection_task = asyncio.create_task(self._collection_loop())
            self.export_task = asyncio.create_task(self._export_loop())
            self.aggregation_task = asyncio.create_task(self._aggregation_loop())
            
            self.logger.info("Coleta de m√©tricas iniciada")
            
        except Exception as e:
            self.logger.error("Erro ao iniciar coleta", error=str(e))
            raise
    
    async def stop_collection(self):
        """Para coleta de m√©tricas"""
        
        try:
            self.logger.info("Parando coleta de m√©tricas")
            self.running = False
            
            # Cancela tasks
            for task in [self.collection_task, self.export_task, self.aggregation_task]:
                if task:
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            self.logger.info("Coleta de m√©tricas parada")
            
        except Exception as e:
            self.logger.error("Erro ao parar coleta", error=str(e))
    
    async def _collection_loop(self):
        """Loop principal de coleta de m√©tricas"""
        
        while self.running:
            try:
                # Coleta m√©tricas do sistema
                system_metrics = await self._collect_system_metrics()
                self.system_metrics_history.append(system_metrics)
                
                # Atualiza m√©tricas Prometheus
                await self._update_prometheus_system_metrics(system_metrics)
                
                # Coleta m√©tricas customizadas
                await self._collect_custom_metrics()
                
                await asyncio.sleep(self.collection_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error("Erro no loop de coleta", error=str(e))
                await asyncio.sleep(30)  # Backoff em caso de erro
    
    async def _collect_system_metrics(self) -> SystemMetrics:
        """Coleta m√©tricas do sistema"""
        
        # CPU
        cpu_percent = psutil.cpu_percent(interval=None)
        
        # Memory
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_gb = memory.used / (1024**3)
        memory_available_gb = memory.available / (1024**3)
        
        # Disk
        disk = psutil.disk_usage('/')
        disk_usage_percent = (disk.used / disk.total) * 100
        disk_free_gb = disk.free / (1024**3)
        
        # Network
        network = psutil.net_io_counters()
        network_bytes_sent = network.bytes_sent
        network_bytes_recv = network.bytes_recv
        
        # Load average
        load_average = list(psutil.getloadavg()) if hasattr(psutil, 'getloadavg') else [0.0, 0.0, 0.0]
        
        return SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            memory_used_gb=memory_used_gb,
            memory_available_gb=memory_available_gb,
            disk_usage_percent=disk_usage_percent,
            disk_free_gb=disk_free_gb,
            network_bytes_sent=network_bytes_sent,
            network_bytes_recv=network_bytes_recv,
            load_average=load_average
        )
    
    async def _update_prometheus_system_metrics(self, metrics: SystemMetrics):
        """Atualiza m√©tricas do sistema no Prometheus"""
        
        self.set_gauge("system_cpu_percent", metrics.cpu_percent)
        self.set_gauge("system_memory_percent", metrics.memory_percent)
        self.set_gauge("system_disk_usage_percent", metrics.disk_usage_percent, {"mount_point": "/"})
        
        # Network deltas se temos dados anteriores
        if self.last_network_stats:
            sent_delta = metrics.network_bytes_sent - self.last_network_stats.network_bytes_sent
            recv_delta = metrics.network_bytes_recv - self.last_network_stats.network_bytes_recv
            
            if sent_delta >= 0:  # Evita valores negativos em case de reset
                self.increment_counter("network_bytes_total", sent_delta, {"direction": "sent"})
            if recv_delta >= 0:
                self.increment_counter("network_bytes_total", recv_delta, {"direction": "received"})
        
        self.last_network_stats = metrics
    
    async def _collect_custom_metrics(self):
        """Coleta m√©tricas customizadas da aplica√ß√£o"""
        
        # ETL memory usage
        process = psutil.Process()
        memory_info = process.memory_info()
        self.set_gauge("etl_memory_usage_bytes", memory_info.rss, {"component": "main"})
        
        # Connection pool metrics se dispon√≠vel
        try:
            # Exemplo: m√©tricas de pool de conex√£o
            active_connections = self._get_active_connections()
            self.set_gauge("database_connections_active", active_connections)
        except:
            pass
    
    def _get_active_connections(self) -> int:
        """Exemplo de coleta de conex√µes ativas"""
        # Implementar baseado no pool de conex√£o usado
        return 0
    
    async def _export_loop(self):
        """Loop para export de m√©tricas para sistemas externos"""
        
        while self.running:
            try:
                # Export para Redis (para dashboards distribu√≠dos)
                await self._export_to_redis()
                
                # Export para outros sistemas se configurado
                await self._export_to_external_systems()
                
                await asyncio.sleep(60)  # Export a cada minuto
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error("Erro no export de m√©tricas", error=str(e))
                await asyncio.sleep(120)
    
    async def _export_to_redis(self):
        """Exporta m√©tricas para Redis"""
        
        try:
            # Export m√©tricas de sistema recentes
            if self.system_metrics_history:
                latest_metrics = self.system_metrics_history[-1]
                
                metrics_data = {
                    'timestamp': latest_metrics.timestamp.isoformat(),
                    'cpu_percent': latest_metrics.cpu_percent,
                    'memory_percent': latest_metrics.memory_percent,
                    'disk_usage_percent': latest_metrics.disk_usage_percent,
                    'service': self.service_name
                }
                
                # Store current metrics
                key = f"metrics:{self.service_name}:system"
                self.redis_client.setex(key, 300, json.dumps(metrics_data))  # 5min TTL
                
                # Store time series
                ts_key = f"metrics_ts:{self.service_name}:system"
                self.redis_client.zadd(ts_key, {
                    json.dumps(metrics_data): latest_metrics.timestamp.timestamp()
                })
                
                # Clean old time series data (>24h)
                cutoff = (datetime.now() - timedelta(hours=24)).timestamp()
                self.redis_client.zremrangebyscore(ts_key, 0, cutoff)
        
        except Exception as e:
            self.logger.error("Erro ao exportar para Redis", error=str(e))
    
    async def _export_to_external_systems(self):
        """Exporta m√©tricas para sistemas externos (InfluxDB, etc.)"""
        
        # Implementar exports espec√≠ficos conforme necess√°rio
        pass
    
    async def _aggregation_loop(self):
        """Loop para agrega√ß√£o de m√©tricas"""
        
        while self.running:
            try:
                await self._aggregate_metrics()
                await asyncio.sleep(300)  # Agrega a cada 5 minutos
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error("Erro na agrega√ß√£o", error=str(e))
                await asyncio.sleep(600)
    
    async def _aggregate_metrics(self):
        """Agrega m√©tricas para an√°lises"""
        
        if len(self.system_metrics_history) < 2:
            return
        
        # Calcula m√©dias dos √∫ltimos 5 minutos
        recent_metrics = [m for m in self.system_metrics_history 
                         if (datetime.now() - m.timestamp).seconds <= 300]
        
        if not recent_metrics:
            return
        
        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
        avg_disk = sum(m.disk_usage_percent for m in recent_metrics) / len(recent_metrics)
        
        # Store aggregated metrics
        aggregated_data = {
            'timestamp': datetime.now().isoformat(),
            'window_minutes': 5,
            'avg_cpu_percent': avg_cpu,
            'avg_memory_percent': avg_memory,
            'avg_disk_percent': avg_disk,
            'samples_count': len(recent_metrics)
        }
        
        agg_key = f"metrics_agg:{self.service_name}:5m"
        self.redis_client.zadd(agg_key, {
            json.dumps(aggregated_data): datetime.now().timestamp()
        })
        
        self.logger.debug("M√©tricas agregadas", data=aggregated_data)
    
    # Public API methods for recording metrics
    
    def increment_counter(self, name: str, value: float = 1.0, labels: Optional[Dict[str, str]] = None):
        """Incrementa contador"""
        
        if name not in self.custom_metrics:
            self.logger.warning("M√©trica n√£o encontrada", name=name)
            return
        
        metric = self.custom_metrics[name]
        if labels:
            metric.labels(**labels).inc(value)
        else:
            metric.inc(value)
        
        # Buffer para processamento posterior
        self.metrics_buffer.append(MetricValue(name, value, labels or {}))
    
    def set_gauge(self, name: str, value: float, labels: Optional[Dict[str, str]] = None):
        """Define valor de gauge"""
        
        if name not in self.custom_metrics:
            self.logger.warning("M√©trica n√£o encontrada", name=name)
            return
        
        metric = self.custom_metrics[name]
        if labels:
            metric.labels(**labels).set(value)
        else:
            metric.set(value)
        
        self.metrics_buffer.append(MetricValue(name, value, labels or {}))
    
    def observe_histogram(self, name: str, value: float, labels: Optional[Dict[str, str]] = None):
        """Observa valor em histograma"""
        
        if name not in self.custom_metrics:
            self.logger.warning("M√©trica n√£o encontrada", name=name)
            return
        
        metric = self.custom_metrics[name]
        if labels:
            metric.labels(**labels).observe(value)
        else:
            metric.observe(value)
        
        self.metrics_buffer.append(MetricValue(name, value, labels or {}))
    
    def time_function(self, metric_name: str, labels: Optional[Dict[str, str]] = None):
        """Decorator para medir tempo de execu√ß√£o"""
        
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    duration = time.time() - start_time
                    self.observe_histogram(metric_name, duration, labels)
            return wrapper
        return decorator
    
    def async_time_function(self, metric_name: str, labels: Optional[Dict[str, str]] = None):
        """Decorator para medir tempo de execu√ß√£o de fun√ß√µes async"""
        
        def decorator(func):
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    return result
                finally:
                    duration = time.time() - start_time
                    self.observe_histogram(metric_name, duration, labels)
            return wrapper
        return decorator
    
    # Business metrics helpers
    
    def record_etl_processing(self, 
                             pipeline: str,
                             stage: str,
                             records_count: int,
                             duration_seconds: float,
                             status: str = "success"):
        """Registra m√©tricas de processamento ETL"""
        
        labels = {"pipeline": pipeline, "stage": stage, "status": status}
        
        self.increment_counter("etl_records_processed_total", records_count, labels)
        self.observe_histogram("etl_processing_duration_seconds", duration_seconds, 
                              {"pipeline": pipeline, "stage": stage})
        
        # Calcula throughput
        if duration_seconds > 0:
            throughput = records_count / duration_seconds
            self.set_gauge("etl_throughput_records_per_second", throughput, {"pipeline": pipeline})
    
    def record_data_quality(self, dataset: str, dimension: str, score: float):
        """Registra m√©tricas de qualidade de dados"""
        
        self.set_gauge("data_quality_score", score, {"dataset": dataset, "dimension": dimension})
    
    def record_business_transaction(self, transaction_type: str, amount: float, currency: str = "BRL"):
        """Registra m√©tricas de transa√ß√µes de neg√≥cio"""
        
        self.increment_counter("business_transactions_amount_total", amount, 
                              {"transaction_type": transaction_type, "currency": currency})
    
    def record_user_processing(self, source: str, count: int = 1):
        """Registra processamento de usu√°rios"""
        
        self.increment_counter("business_users_processed_total", count, {"source": source})
    
    # Query and analysis methods
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """Retorna m√©tricas atuais"""
        
        current_metrics = {}
        
        # System metrics
        if self.system_metrics_history:
            latest_system = self.system_metrics_history[-1]
            current_metrics['system'] = {
                'cpu_percent': latest_system.cpu_percent,
                'memory_percent': latest_system.memory_percent,
                'disk_usage_percent': latest_system.disk_usage_percent,
                'timestamp': latest_system.timestamp.isoformat()
            }
        
        # Application metrics from buffer
        if self.metrics_buffer:
            recent_app_metrics = list(self.metrics_buffer)[-50:]  # Last 50 metrics
            current_metrics['application'] = [
                {
                    'name': m.metric_name,
                    'value': m.value,
                    'labels': m.labels,
                    'timestamp': m.timestamp.isoformat()
                }
                for m in recent_app_metrics
            ]
        
        return current_metrics
    
    def get_metrics_summary(self, hours: int = 1) -> Dict[str, Any]:
        """Retorna resumo de m√©tricas"""
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # System metrics summary
        recent_system = [m for m in self.system_metrics_history if m.timestamp >= cutoff_time]
        
        system_summary = {}
        if recent_system:
            system_summary = {
                'avg_cpu_percent': sum(m.cpu_percent for m in recent_system) / len(recent_system),
                'max_cpu_percent': max(m.cpu_percent for m in recent_system),
                'avg_memory_percent': sum(m.memory_percent for m in recent_system) / len(recent_system),
                'max_memory_percent': max(m.memory_percent for m in recent_system),
                'samples_count': len(recent_system)
            }
        
        # Application metrics summary
        recent_app = [m for m in self.metrics_buffer if m.timestamp >= cutoff_time]
        
        app_summary = {}
        if recent_app:
            metrics_by_name = defaultdict(list)
            for metric in recent_app:
                metrics_by_name[metric.metric_name].append(metric.value)
            
            app_summary = {
                name: {
                    'count': len(values),
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values)
                }
                for name, values in metrics_by_name.items()
            }
        
        return {
            'time_window_hours': hours,
            'system_metrics': system_summary,
            'application_metrics': app_summary,
            'generated_at': datetime.now().isoformat()
        }
    
    def export_prometheus_metrics(self) -> str:
        """Exporta m√©tricas no formato Prometheus"""
        
        return generate_latest(self.registry).decode('utf-8')
    
    def get_health_status(self) -> Dict[str, Any]:
        """Retorna status de sa√∫de baseado em m√©tricas"""
        
        status = {
            'healthy': True,
            'issues': [],
            'last_check': datetime.now().isoformat()
        }
        
        if self.system_metrics_history:
            latest = self.system_metrics_history[-1]
            
            # Check thresholds
            if latest.cpu_percent > 90:
                status['healthy'] = False
                status['issues'].append(f"High CPU usage: {latest.cpu_percent:.1f}%")
            
            if latest.memory_percent > 90:
                status['healthy'] = False
                status['issues'].append(f"High memory usage: {latest.memory_percent:.1f}%")
            
            if latest.disk_usage_percent > 90:
                status['healthy'] = False
                status['issues'].append(f"High disk usage: {latest.disk_usage_percent:.1f}%")
            
            # Check data freshness
            data_age = (datetime.now() - latest.timestamp).seconds
            if data_age > 300:  # 5 minutes
                status['healthy'] = False
                status['issues'].append(f"Stale metrics data: {data_age}s old")
        
        return status

# Context manager para uso autom√°tico
class MetricsContext:
    """Context manager para coletor de m√©tricas"""
    
    def __init__(self, service_name: str = "crmbet-etl", prometheus_port: int = 9090):
        self.collector = MetricsCollector(service_name, prometheus_port)
    
    async def __aenter__(self):
        await self.collector.start_collection()
        return self.collector
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.collector.stop_collection()

# Global collector instance para uso simples
_global_collector: Optional[MetricsCollector] = None

def get_metrics_collector() -> MetricsCollector:
    """Retorna inst√¢ncia global do coletor"""
    global _global_collector
    
    if _global_collector is None:
        _global_collector = MetricsCollector()
    
    return _global_collector

# Decorators convenientes
def measure_time(metric_name: str, labels: Optional[Dict[str, str]] = None):
    """Decorator para medir tempo de execu√ß√£o"""
    collector = get_metrics_collector()
    return collector.time_function(metric_name, labels)

def async_measure_time(metric_name: str, labels: Optional[Dict[str, str]] = None):
    """Decorator para medir tempo de execu√ß√£o async"""
    collector = get_metrics_collector()
    return collector.async_time_function(metric_name, labels)